#!/bin/bash

print_usage() {
cat <<EOF
Usage: $0 <command> [-options] [args...]

commands:
  spark-streaming [-options] <jar>           submit a Spark Streaming job

  create-table [-options] <table spec file>  create hive table
  etl [-options] <jar> <logdate>             submit a etl job

  generate-project <project name>            generate base project structure
  publish-package <package name> <jar>       publish user defined package with group:artifact:version name format

  spark-submit [spark options]               execute spark-submit command
  spark-shell [spark options]                execute spark-shell command
  spark-sql [spark options]                  execute spark-sql command

  help                                       display this help text


general options:
  -Dpackages=<package list> specify comma-separated list, like group1:artifact1:version1,group2:artifact2:version2

spark-streaming options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Dapproach=<Dapproach>    specify the Kafka approach, "receiver-based" or "direct-based"
  -DzkConnString=<url>      specify the ZooKeeper connection to use
  -Dtopics=<topics>         specify the Kafka topics with comma seperated
  -DgroupId=<group>         specify the Kafka consumer group id
  -DbatchDuration=<seconds> specify the Spark Streaming batch process duration
  -Dprocessor=<class>       specify the processor class
  -DthreadNum=<num>         specify the Kafka consumer thread num (only need for "receiver-based")
  -DbrokerList=<url>        specify the kafka broker list (only need for "direct-based")
  -DoffsetsCommitBatchInterval=<interval>  specify the Kafka offsets commit batch interval  (only need for "direct-based")
  -D<property>=<value>      specify any Java system property value

etl options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Ddatabase=<database>     specify the database, default custom
  -Dtable=<table>           specify the table
  -Dprocessor=<class>       specify the processor class
  -Dparallelism=<num>       specify the parallelism which will influence orc file num and write parallelism, default 1

EOF
}

submit_spark_streaming() {
  userJar=$ARGS
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi

  LOCAL_CLASSPATH=$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$SPARK_CLASSPATH:$USER_JAR:$PACKAGES_CLASSPATH
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR:$PACKAGES_CLASSPATH`
  JAVA_OPTS=`echo " $OPTIONS" | sed "s/[ \t]-D/ -Droot.sparkStreaming./g"`
  MAIN_CLASS="com.sogou.bigdatakit.streaming.SparkStreaming"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH $JAVA_OPTS -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS
}

create_table() {
  LOCAL_CLASSPATH=$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$SPARK_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH
  JAVA_OPTS=$OPTIONS
  MAIN_CLASS="com.sogou.bigdatakit.hive.CreateTable"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -XX:MaxPermSize=256m $JAVA_OPTS -cp $LOCAL_CLASSPATH $MAIN_CLASS $ARGS
}

etl() {
  userJar=`echo $ARGS | awk '{print $1}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi
  if [ $ARGS_NUM -eq 2 ]; then
    logdate=`echo $ARGS| awk '{print $2}'`
  elif [ $ARGS_NUM -eq 3 ]; then
    logid=`echo $ARGS| awk '{print $2}'`
    logdate=`echo $ARGS| awk '{print $3}'`
  fi

  LOCAL_CLASSPATH=$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$SPARK_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  JAVA_OPTS=`echo " $OPTIONS" | sed "s/[ \t]-D/ -Droot.hive.etl./g"`
  MAIN_CLASS="com.sogou.bigdatakit.hive.etl.ETL"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -Xmx1G -XX:MaxPermSize=256M $JAVA_OPTS -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS $logdate
}

generate_project() {
  name=`echo $ARGS| awk '{print $1}'`
  git clone $BIGDATAKIT_STARTUP_GIT_URL $name
  sed -i "s/\${artifactId}/$name/g" $name/pom.xml
  rm -fr $name/.git
}

publish_package() {
  name=`echo $ARGS| awk '{print $1}'`
  groupId=`echo $name | awk -F":" '{print $1}'`
  artifactId=`echo $name | awk -F":" '{print $2}'`
  version=`echo $name | awk -F":" '{print $3}'`
  if [ X$groupId == X ] || [ X$artifactId == X ] || [ X$version == X ]; then
    echo "groupId, artifactId or version is empty"
    return 1
  fi
  userJar=`echo $ARGS | awk '{print $2}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi

  HDFS_PACKAGE_DIR=$BIGDATAKIT_HDFS_PACKAGE_DIR/$groupId/$artifactId/$version
  HDFS_PACKAGE_JAR=$HDFS_PACKAGE_DIR/$artifactId-$version.jar
  hadoop fs -mkdir -p $HDFS_PACKAGE_DIR
  hadoop fs -rm $HDFS_PACKAGE_JAR
  echo "try to upload $groupId:$artifactId:$version"
  hadoop fs -copyFromLocal $USER_JAR $HDFS_PACKAGE_JAR
  hadoop fs -chmod -R +w $HDFS_PACKAGE_DIR
  echo "package $groupId:$artifactId:$version pushlish succeed"
}

spark_submit() {
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  $SPARK_HOME/bin/spark-submit $FULL_PARAMS
}

spark_shell() {
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  $SPARK_HOME/bin/spark-shell $FULL_PARAMS
}

spark_sql() {
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  $SPARK_HOME/bin/spark-sql $FULL_PARAMS
}

classpath_to_sparkjars() {
  echo $1 | awk 'BEGIN{RS=":";ORS=","}{if($1!=""){print "file://"$1}}'
}

get_absolute_path() {
  path=$1
  if [[ `ls $path >/dev/null 2>&1; echo $?` -ne 0 ]]; then
    return 1
  fi
  readlink -f $path
}

parse_params() {
  COMMAND=$1; shift
  while (($#)); do
    if [[ "$1" =~ ^"-" ]]; then
      if [[ "$1" =~ ^"-Dpackages=" ]]; then
        PACKAGES=`echo $1 | awk -F"=" '{print $2}'`
      else
        OPTIONS="$OPTIONS $1"
        FULL_PARAMS="$FULL_PARAMS $1"
      fi
    else
      ARGS="$ARGS $1"
      ARGS_NUM=`expr $ARGS_NUM + 1`
      FULL_PARAMS="$FULL_PARAMS $1"
    fi
    shift
  done
}

download_packages() {
  packages=$1
  echo -n $packages | awk 'BEGIN{RS=","}{print $0}' >.packages_list
  while read name
  do
    groupId=`echo $name | awk -F":" '{print $1}'`
    artifactId=`echo $name | awk -F":" '{print $2}'`
    version=`echo $name | awk -F":" '{print $3}'`
    if [ X$groupId == X ] || [ X$artifactId == X ] || [ X$version == X ]; then
      echo "invalid package format: $name, need to be group:artifact:version"
      continue
    fi
    HDFS_PACKAGE_DIR=$BIGDATAKIT_HDFS_PACKAGE_DIR/$groupId/$artifactId/$version
    HDFS_PACKAGE_JAR=$HDFS_PACKAGE_DIR/$artifactId-$version.jar
    LOCAL_PACKAGE_DIR=$BIGDATAKIT_LOCAL_PACKAGE_DIR/$groupId/$artifactId/$version
    LOCAL_PACKAGE_JAR=$LOCAL_PACKAGE_DIR/$artifactId-$version.jar
    mkdir -p $LOCAL_PACKAGE_DIR
    HDFS_PACKAGE_SIZE=`hadoop fs -ls $HDFS_PACKAGE_JAR 2>/dev/null | awk '{print $5}'`
    LOCAL_PACKAGE_SIZE=`ls -l $LOCAL_PACKAGE_JAR 2>/dev/null | awk '{print $5}'`
    if [ X$HDFS_PACKAGE_SIZE == X ]; then
      echo "$groupId:$artifactId:$version is not found"
      continue
    fi
    if [ X$LOCAL_PACKAGE_SIZE == X ] || [ $LOCAL_PACKAGE_SIZE -ne $HDFS_PACKAGE_SIZE ]; then
      echo "try to download $groupId:$artifactId:$version"
      rm -f $LOCAL_PACKAGE_JAR
      hadoop fs -copyToLocal $HDFS_PACKAGE_JAR $LOCAL_PACKAGE_DIR
      echo "package $groupId:$artifactId:$version download succeed"
    fi
    PACKAGES_CLASSPATH=$PACKAGES_CLASSPATH:$LOCAL_PACKAGE_JAR
  done <.packages_list
  rm -f .packages_list
}

dir=`dirname $0`
dir=`cd $dir/..; pwd`
. $dir/conf/bigdatakit-env.sh

COMMAND=""
OPTIONS=""
ARGS=""
ARGS_NUM=0
PACKAGES=""
PACKAGES_CLASSPATH=""
FULL_PARAMS=""
parse_params $@
download_packages $PACKAGES

case $COMMAND in
  help)
    print_usage
    exit 0
    ;;

  spark-streaming)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 spark-streaming [-options] <jar>"
      exit 1
    fi
    submit_spark_streaming
    exit $?
    ;;

  create-table)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 create-table [-options] <table spec file>"
      exit 1
    fi
    create_table
    exit $?
    ;;

  etl)
    if [ $ARGS_NUM -ne 2 ] && [ $ARGS_NUM -ne 3 ]; then
      echo "Usage: $0 etl [-options] <jar> <logdate>"
      exit 1
    fi
    etl
    exit $?
    ;;

  generate-project)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 generate-project <project name>"
      exit 1
    fi
    generate_project
    exit $?
    ;;

  publish-package)
    if [ $ARGS_NUM -ne 2 ]; then
      echo "Usage: $0 publish-package <group:artifact:version> <jar>"
      exit 1
    fi
    publish_package
    exit $?
    ;;

  spark-shell)
    shift
    spark_shell $@
    exit $?
    ;;

  spark-submit)
    shift
    spark_submit $@
    exit $?
    ;;

  spark-sql)
    shift
    spark_sql $@
    exit $?
    ;;

  *)
    print_usage
    exit 1
    ;;
esac