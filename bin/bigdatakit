#!/bin/bash

print_usage() {
cat <<EOF
Usage: $0 <command> [-options] [args...]

commands:
  spark-streaming [-options] <jar>           submit a Spark Streaming job

  create-table [-options] <table spec file>  create hive table
  etl [-options] <jar> <logdate>             submit a etl job

  publish-package <group:artifact:version> <jar>   publish user package

  spark-submit [spark options]               execute spark-submit command
  spark-shell [spark options]                execute spark-shell command
  spark-sql [spark options]                  execute spark-sql command

  help                                       display this help text


spark-streaming options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Dapproach=<Dapproach>    specify the Kafka approach, "receiver-based" or "direct-based"
  -DzkConnString=<url>      specify the ZooKeeper connection to use
  -Dtopics=<topics>         specify the Kafka topics with comma seperated
  -DgroupId=<group>         specify the Kafka consumer group id
  -DbatchDuration=<seconds> specify the Spark Streaming batch process duration
  -Dprocessor=<class>       specify the processor class
  -DthreadNum=<num>         specify the Kafka consumer thread num (only need for "receiver-based")
  -DbrokerList=<url>        specify the kafka broker list (only need for "direct-based")
  -DoffsetsCommitBatchInterval=<interval>  specify the Kafka offsets commit batch interval  (only need for "direct-based")
  -D<property>=<value>      specify any Java system property value

etl options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Dname=<database>         specify the database
  -Dname=<table>            specify the table
  -Dprocessor=<class>       specify the processor class

EOF
}

submit_spark_streaming() {
  userJar=$ARGS
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi

  LOCAL_CLASSPATH=$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$SPARK_CLASSPATH:$USER_JAR
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR`
  JAVA_OPTS=`echo " $OPTIONS" | sed "s/[ \t]-D/ -Droot.sparkStreaming./g"`
  MAIN_CLASS="com.sogou.bigdatakit.streaming.SparkStreaming"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH $JAVA_OPTS -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS
}

create_table() {
  LOCAL_CLASSPATH=$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$SPARK_CLASSPATH:$HIVE_AUXLIB_CLASSPATH
  JAVA_OPTS=$OPTIONS
  MAIN_CLASS="com.sogou.bigdatakit.hive.CreateTable"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -XX:MaxPermSize=256m $JAVA_OPTS -cp $LOCAL_CLASSPATH $MAIN_CLASS $ARGS
}

etl() {
  userJar=`echo $ARGS | awk '{print $1}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi
  if [ $ARGS_NUM -eq 2 ]; then
    logdate=`echo $ARGS| awk '{print $2}'`
  elif [ $ARGS_NUM -eq 3 ]; then
    logid=`echo $ARGS| awk '{print $2}'`
    logdate=`echo $ARGS| awk '{print $3}'`
  fi

  LOCAL_CLASSPATH=$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$SPARK_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH`
  JAVA_OPTS=`echo " $OPTIONS" | sed "s/[ \t]-D/ -Droot.hive.etl./g"`
  MAIN_CLASS="com.sogou.bigdatakit.hive.etl.ETL"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -Xmx1G -XX:MaxPermSize=256M $JAVA_OPTS -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS $logdate
}

publish_package() {
  name=`echo $ARGS| awk '{print $1}'`
  groupId=`echo $name | awk -F":" '{print $1}'`
  artifactId=`echo $name | awk -F":" '{print $2}'`
  version=`echo $name | awk -F":" '{print $3}'`
  if [ X$groupId == X ] || [ X$artifactId == X ] || [ X$version == X ]; then
    echo "groupId, artifactId or version is empty"
    return 1
  fi
  userJar=`echo $ARGS | awk '{print $2}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi

  HDFS_PACKAGE_DIR=$BIGDATAKIT_HDFS_PACKAGE_DIR/$groupId/$artifactId/$version
  HDFS_PACKAGE_JAR=$HDFS_PACKAGE_DIR/$artifactId-$version.jar
  hadoop fs -mkdir -p $HDFS_PACKAGE_DIR
  hadoop fs -rm $HDFS_PACKAGE_JAR
  echo "try to upload to $HDFS_PACKAGE_JAR"
  hadoop fs -copyFromLocal $USER_JAR $HDFS_PACKAGE_JAR
  hadoop fs -chmod -R +w $HDFS_PACKAGE_DIR
  echo "package $groupId:$artifactId:$version pushlish succeed"
}

spark_submit() {
  $SPARK_HOME/bin/spark-submit $@
}

spark_shell() {
  $SPARK_HOME/bin/spark-shell $@
}

spark_sql() {
  $SPARK_HOME/bin/spark-sql $@
}

classpath_to_sparkjars() {
  echo $1 | awk 'BEGIN{RS=":";ORS=","}{if($1!=""){print "file://"$1}}'
}

get_absolute_path() {
  path=$1
  if [[ `ls $path >/dev/null 2>&1; echo $?` -ne 0 ]]; then
    return 1
  fi
  readlink -f $path
}

parse_params() {
  COMMAND=$1; shift
  while (($#)); do
    if [[ "$1" =~ ^"-" ]]; then
      OPTIONS="$OPTIONS $1"
    else
      ARGS="$ARGS $1"
      ARGS_NUM=`expr $ARGS_NUM + 1`
    fi
    shift
  done
}

dir=`dirname $0`
dir=`cd $dir/..; pwd`
. $dir/conf/bigdatakit-env.sh

COMMAND=""
OPTIONS=""
ARGS=""
ARGS_NUM=0
parse_params $@

case $COMMAND in
  help)
    print_usage
    exit 0
    ;;

  spark-streaming)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 spark-streaming [-options] <jar>"
      exit 1
    fi
    submit_spark_streaming
    exit $?
    ;;

  create-table)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 create-table [-options] <table spec file>"
      exit 1
    fi
    create_table
    exit $?
    ;;

  etl)
    if [ $ARGS_NUM -ne 2 ] && [ $ARGS_NUM -ne 3 ]; then
      echo "Usage: $0 etl [-options] <jar> <logdate>"
      exit 1
    fi
    etl
    exit $?
    ;;

  publish-package)
    if [ $ARGS_NUM -ne 2 ]; then
      echo "Usage: $0 publish-package <group:artifact:version> <jar>"
      exit 1
    fi
    publish_package
    exit $?
    ;;

  spark-shell)
    shift
    spark_shell $@
    exit $?
    ;;

  spark-submit)
    shift
    spark_submit $@
    exit $?
    ;;

  spark-sql)
    shift
    spark_sql $@
    exit $?
    ;;

  *)
    print_usage
    exit 1
    ;;
esac