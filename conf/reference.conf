root {
  spark {
    spark.shuffle.manager = SORT
    spark.shuffle.consolidateFiles = true
    spark.io.compression.codec = org.apache.spark.io.LZ4CompressionCodec
    spark.scheduler.mode = FAIR
    spark.speculation = false
    spark.yarn.jar = "viewfs://nsX/user/spark/lib/spark-assembly-1.5.3-SNAPSHOT-hadoop2.5.0-cdh5.3.2.jar"

    spark.eventLog.enabled = true
    spark.eventLog.compress = true
    spark.eventLog.dir = "viewfs://nsX/tmp/spark-events"

    spark.history.fs.cleaner.enabled = true
    spark.history.fs.cleaner.interval = 1d
    spark.history.fs.cleaner.maxAge = 7d
    spark.history.fs.logDirectory = "viewfs://nsX/tmp/spark-events"
    spark.yarn.historyServer.address = "rsync.historyserver01.spark.user.nop.sogou-op.org:18080"

    spark.sql.planner.externalSort = true
    spark.sql.hive.metastorePartitionPruning = true
    spark.sql.tungsten.enabled = false
  }

  sparkStreaming {
    master = "local[*]"
    name = "bigdatakit-spark-streaming"
    approach = "receiver-based"
    zkConnString = "rsync.broker01.kafka01.user.nop:2182,rsync.broker02.kafka01.user.nop:2182,rsync.broker03.kafka01.user.nop:2182/kafka"
    brokerList = "rsync.broker01.kafka01.user.nop:9092,rsync.broker02.kafka01.user.nop:9092,rsync.broker03.kafka01.user.nop:9092"
    zkSessionTimeout = 6000
    zkConnectionTimeout = 6000
    offsetsCommitBatchInterval = 6
    threadNum = 1
    batchDuration = 10
  }

  hive.etl {
    master = "local[*]"
  }

  hbase.etl {
    master = "local[*]"
  }
}