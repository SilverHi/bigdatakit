root {
  spark {
    spark.shuffle.manager = SORT
    spark.shuffle.consolidateFiles = true
    spark.io.compression.codec = org.apache.spark.io.LZ4CompressionCodec
    spark.scheduler.mode = FAIR
    spark.speculation = false
    spark.yarn.jar = "viewfs://nsX/user/spark/lib/spark-assembly-1.5.3-SNAPSHOT-hadoop2.5.0-cdh5.3.2.jar"

    spark.eventLog.enabled = true
    spark.eventLog.compress = true
    spark.eventLog.dir = "viewfs://nsX/tmp/spark-events"

    spark.history.fs.cleaner.enabled = true
    spark.history.fs.cleaner.interval = 1d
    spark.history.fs.cleaner.maxAge = 7d
    spark.history.fs.logDirectory = "viewfs://nsX/tmp/spark-events"
    spark.yarn.historyServer.address = "rsync.historyserver01.spark.user.nop.sogou-op.org:18080"

    spark.sql.planner.externalSort = true
    spark.sql.hive.metastorePartitionPruning = true
    spark.sql.tungsten.enabled = false
  }

  sparkStreaming {
    master = "local[*]"
    name = "bigdatakit-spark-streaming"
    zkConnString = "cloud204097.wd.cnc.ss.nop.sogou-op.org:2182,cloud204098.wd.cnc.ss.nop.sogou-op.org:2182,cloud204099.wd.cnc.ss.nop.sogou-op.org:2182,cloud204085.wd.cnc.ss.nop.sogou-op.org:2182/kafka"
    threadNum = 1
    batchDuration = 10
  }
}